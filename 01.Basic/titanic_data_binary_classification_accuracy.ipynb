{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"titanic_data_binary_classification_accuracy.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO5lna1xzj647gr5NNIKP9D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Titanic Data Binary Classification Accuracy | 타이타닉 데이터 이진 분류 정확도"],"metadata":{"id":"vWUPuGcZ_yzF"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"xs3gOLeapYSs","executionInfo":{"status":"ok","timestamp":1641947958651,"user_tz":-540,"elapsed":3028,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}}},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import seaborn as sns\n","import tensorflow as tf\n","seed = 2021\n","np.random.seed(seed)\n","tf.random.set_seed(seed)"]},{"cell_type":"markdown","source":["## 데이터 전처리"],"metadata":{"id":"rjEXF60w_wKZ"}},{"cell_type":"code","source":["df = sns.load_dataset('titanic')\n","df.head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"xuWzQa5Rpt51","executionInfo":{"status":"ok","timestamp":1641947959617,"user_tz":-540,"elapsed":968,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"outputId":"8e94d8b4-8d48-4eff-e886-61ebd8feb2bf"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-e5d9cbda-5be1-4018-8fc0-229ca22fa350\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>survived</th>\n","      <th>pclass</th>\n","      <th>sex</th>\n","      <th>age</th>\n","      <th>sibsp</th>\n","      <th>parch</th>\n","      <th>fare</th>\n","      <th>embarked</th>\n","      <th>class</th>\n","      <th>who</th>\n","      <th>adult_male</th>\n","      <th>deck</th>\n","      <th>embark_town</th>\n","      <th>alive</th>\n","      <th>alone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7.2500</td>\n","      <td>S</td>\n","      <td>Third</td>\n","      <td>man</td>\n","      <td>True</td>\n","      <td>NaN</td>\n","      <td>Southampton</td>\n","      <td>no</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>71.2833</td>\n","      <td>C</td>\n","      <td>First</td>\n","      <td>woman</td>\n","      <td>False</td>\n","      <td>C</td>\n","      <td>Cherbourg</td>\n","      <td>yes</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.9250</td>\n","      <td>S</td>\n","      <td>Third</td>\n","      <td>woman</td>\n","      <td>False</td>\n","      <td>NaN</td>\n","      <td>Southampton</td>\n","      <td>yes</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5d9cbda-5be1-4018-8fc0-229ca22fa350')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e5d9cbda-5be1-4018-8fc0-229ca22fa350 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e5d9cbda-5be1-4018-8fc0-229ca22fa350');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   survived  pclass     sex   age  ...  deck  embark_town  alive  alone\n","0         0       3    male  22.0  ...   NaN  Southampton     no  False\n","1         1       1  female  38.0  ...     C    Cherbourg    yes  False\n","2         1       3  female  26.0  ...   NaN  Southampton    yes   True\n","\n","[3 rows x 15 columns]"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# 결측치 확인\n","df.isna().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gFzNdlVcqkir","executionInfo":{"status":"ok","timestamp":1641947959617,"user_tz":-540,"elapsed":5,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"outputId":"a900f4e6-4708-4489-f2ce-c5e08d5923ae"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["survived         0\n","pclass           0\n","sex              0\n","age            177\n","sibsp            0\n","parch            0\n","fare             0\n","embarked         2\n","class            0\n","who              0\n","adult_male       0\n","deck           688\n","embark_town      2\n","alive            0\n","alone            0\n","dtype: int64"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["df = df[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'deck']]\n","df"],"metadata":{"id":"s9NywG2qqqE3","executionInfo":{"status":"ok","timestamp":1641947960410,"user_tz":-540,"elapsed":30,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"colab":{"base_uri":"https://localhost:8080/","height":424},"outputId":"973a6d84-b7da-4e04-b7a1-2eb14c22329b"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-6761d4e9-7049-4c42-b46b-f9306f7da5fe\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>survived</th>\n","      <th>pclass</th>\n","      <th>sex</th>\n","      <th>age</th>\n","      <th>sibsp</th>\n","      <th>parch</th>\n","      <th>fare</th>\n","      <th>embarked</th>\n","      <th>deck</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7.2500</td>\n","      <td>S</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>71.2833</td>\n","      <td>C</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.9250</td>\n","      <td>S</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>53.1000</td>\n","      <td>S</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8.0500</td>\n","      <td>S</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>886</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>male</td>\n","      <td>27.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>13.0000</td>\n","      <td>S</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>887</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>female</td>\n","      <td>19.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30.0000</td>\n","      <td>S</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>888</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>female</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>23.4500</td>\n","      <td>S</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>889</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>male</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30.0000</td>\n","      <td>C</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>890</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>male</td>\n","      <td>32.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.7500</td>\n","      <td>Q</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>891 rows × 9 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6761d4e9-7049-4c42-b46b-f9306f7da5fe')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6761d4e9-7049-4c42-b46b-f9306f7da5fe button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6761d4e9-7049-4c42-b46b-f9306f7da5fe');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["     survived  pclass     sex   age  sibsp  parch     fare embarked deck\n","0           0       3    male  22.0      1      0   7.2500        S  NaN\n","1           1       1  female  38.0      1      0  71.2833        C    C\n","2           1       3  female  26.0      0      0   7.9250        S  NaN\n","3           1       1  female  35.0      1      0  53.1000        S    C\n","4           0       3    male  35.0      0      0   8.0500        S  NaN\n","..        ...     ...     ...   ...    ...    ...      ...      ...  ...\n","886         0       2    male  27.0      0      0  13.0000        S  NaN\n","887         1       1  female  19.0      0      0  30.0000        S    B\n","888         0       3  female   NaN      1      2  23.4500        S  NaN\n","889         1       1    male  26.0      0      0  30.0000        C    C\n","890         0       3    male  32.0      0      0   7.7500        Q  NaN\n","\n","[891 rows x 9 columns]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["df.age.fillna(df.age.mean(), inplace=True)"],"metadata":{"id":"4pNQanupoSBL","executionInfo":{"status":"ok","timestamp":1641947960410,"user_tz":-540,"elapsed":29,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df.embarked.fillna('S', inplace=True)\n","df.embarked.isna().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bHsgDfI9od0U","executionInfo":{"status":"ok","timestamp":1641947960411,"user_tz":-540,"elapsed":30,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"outputId":"74f8ac81-59e3-474d-f352-eae40b1d9b7b"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["df.head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"BvYlKxyvsLNG","executionInfo":{"status":"ok","timestamp":1641947960411,"user_tz":-540,"elapsed":28,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"outputId":"1f1cbbe5-6394-44ae-a069-bceaab7f3770"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-cd6cf16c-fd7f-4ace-9df4-573a51e49e63\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>survived</th>\n","      <th>pclass</th>\n","      <th>sex</th>\n","      <th>age</th>\n","      <th>sibsp</th>\n","      <th>parch</th>\n","      <th>fare</th>\n","      <th>embarked</th>\n","      <th>deck</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7.2500</td>\n","      <td>S</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>71.2833</td>\n","      <td>C</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.9250</td>\n","      <td>S</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd6cf16c-fd7f-4ace-9df4-573a51e49e63')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cd6cf16c-fd7f-4ace-9df4-573a51e49e63 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cd6cf16c-fd7f-4ace-9df4-573a51e49e63');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   survived  pclass     sex   age  sibsp  parch     fare embarked deck\n","0         0       3    male  22.0      1      0   7.2500        S  NaN\n","1         1       1  female  38.0      1      0  71.2833        C    C\n","2         1       3  female  26.0      0      0   7.9250        S  NaN"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["df.drop(columns=['deck'], inplace=True)"],"metadata":{"id":"1VUbjz8do9pk","executionInfo":{"status":"ok","timestamp":1641947960412,"user_tz":-540,"elapsed":28,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# 카테고리형 데이터 cat.codes로 변환\n","df['sex'] = df['sex'].astype('category').cat.codes\n","df['embarked'] = df['embarked'].astype('category').cat.codes\n","\n","# one-hot 인코딩을 위한 get_dummies 사용\n","df['pclass'] = pd.get_dummies(df['pclass'])\n","\n","# 가족 단위 수 group_num을 만들어 새로운 열로 추가\n","df['group_num'] = df['sibsp'] + df['parch'] + 1"],"metadata":{"id":"R9agFSTUqeLk","executionInfo":{"status":"ok","timestamp":1641947960412,"user_tz":-540,"elapsed":28,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["df.head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"G1H2CPBytaVz","executionInfo":{"status":"ok","timestamp":1641947960412,"user_tz":-540,"elapsed":28,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"outputId":"b5ca5bf1-60e5-4b26-ebc5-35bfe7b96cea"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-1e466a78-72e9-4c80-9473-8f3ded69fed6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>survived</th>\n","      <th>pclass</th>\n","      <th>sex</th>\n","      <th>age</th>\n","      <th>sibsp</th>\n","      <th>parch</th>\n","      <th>fare</th>\n","      <th>embarked</th>\n","      <th>group_num</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7.2500</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>71.2833</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.9250</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e466a78-72e9-4c80-9473-8f3ded69fed6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1e466a78-72e9-4c80-9473-8f3ded69fed6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1e466a78-72e9-4c80-9473-8f3ded69fed6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   survived  pclass  sex   age  sibsp  parch     fare  embarked  group_num\n","0         0       0    1  22.0      1      0   7.2500         2          2\n","1         1       1    0  38.0      1      0  71.2833         0          2\n","2         1       0    0  26.0      0      0   7.9250         2          1"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["np.unique(df.values[:,0], return_counts=True) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iu6hg_MWtdUT","executionInfo":{"status":"ok","timestamp":1641947960413,"user_tz":-540,"elapsed":28,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"outputId":"95f4be78-aab6-4e2e-9608-77a01d64f136"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([0., 1.]), array([549, 342]))"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["df.iloc[:, 1:]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"F2Mb-iljtrGD","executionInfo":{"status":"ok","timestamp":1641947960413,"user_tz":-540,"elapsed":26,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"outputId":"f21e745a-92db-43d2-dde1-09f6c5f762f3"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-1b70a184-dd86-4ed4-be5e-d23b025bba08\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pclass</th>\n","      <th>sex</th>\n","      <th>age</th>\n","      <th>sibsp</th>\n","      <th>parch</th>\n","      <th>fare</th>\n","      <th>embarked</th>\n","      <th>group_num</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>22.000000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7.2500</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>38.000000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>71.2833</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>26.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.9250</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>35.000000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>53.1000</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>35.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8.0500</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>886</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>27.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>13.0000</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>887</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>19.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30.0000</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>888</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>29.699118</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>23.4500</td>\n","      <td>2</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>889</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>26.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30.0000</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>890</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>32.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.7500</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>891 rows × 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b70a184-dd86-4ed4-be5e-d23b025bba08')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1b70a184-dd86-4ed4-be5e-d23b025bba08 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1b70a184-dd86-4ed4-be5e-d23b025bba08');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["     pclass  sex        age  sibsp  parch     fare  embarked  group_num\n","0         0    1  22.000000      1      0   7.2500         2          2\n","1         1    0  38.000000      1      0  71.2833         0          2\n","2         0    0  26.000000      0      0   7.9250         2          1\n","3         1    0  35.000000      1      0  53.1000         2          2\n","4         0    1  35.000000      0      0   8.0500         2          1\n","..      ...  ...        ...    ...    ...      ...       ...        ...\n","886       0    1  27.000000      0      0  13.0000         2          1\n","887       1    0  19.000000      0      0  30.0000         2          1\n","888       0    0  29.699118      1      2  23.4500         2          4\n","889       1    1  26.000000      0      0  30.0000         0          1\n","890       0    1  32.000000      0      0   7.7500         1          1\n","\n","[891 rows x 8 columns]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# 데이터 값의 단위가 상이하므로 정규화 수행\n","from sklearn.preprocessing import StandardScaler\n","X_scaled = StandardScaler().fit_transform(df.values[:,1:])\n","X_scaled"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L06QCS0qthzU","executionInfo":{"status":"ok","timestamp":1641947960414,"user_tz":-540,"elapsed":24,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"outputId":"88daf8a5-09bf-42e9-a9ff-7c3a8b974b0f"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.56568542,  0.73769513, -0.5924806 , ..., -0.50244517,\n","         0.58595414,  0.05915988],\n","       [ 1.76776695, -1.35557354,  0.63878901, ...,  0.78684529,\n","        -1.9423032 ,  0.05915988],\n","       [-0.56568542, -1.35557354, -0.2846632 , ..., -0.48885426,\n","         0.58595414, -0.56097483],\n","       ...,\n","       [-0.56568542, -1.35557354,  0.        , ..., -0.17626324,\n","         0.58595414,  1.29942929],\n","       [ 1.76776695,  0.73769513, -0.2846632 , ..., -0.04438104,\n","        -1.9423032 , -0.56097483],\n","       [-0.56568542,  0.73769513,  0.17706291, ..., -0.49237783,\n","        -0.67817453, -0.56097483]])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["y = df.survived\n","y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MU7NgxN4uVbc","executionInfo":{"status":"ok","timestamp":1641947960414,"user_tz":-540,"elapsed":20,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"outputId":"4066116d-0663-41c2-fbb8-b80b35afbe8b"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0      0\n","1      1\n","2      1\n","3      1\n","4      0\n","      ..\n","886    0\n","887    1\n","888    0\n","889    1\n","890    0\n","Name: survived, Length: 891, dtype: int64"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["## 모델 정의/설정"],"metadata":{"id":"dj-zvQ1EAtFa"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_scaled, y, stratify=y, random_state=seed, test_size=0.2\n",")\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gQT6sO47tp_b","executionInfo":{"status":"ok","timestamp":1641947960829,"user_tz":-540,"elapsed":433,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"outputId":"6cc88157-c108-43d4-846e-286d31db99cb"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((712, 8), (179, 8), (712,), (179,))"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense"],"metadata":{"id":"Winh6exPuHKU","executionInfo":{"status":"ok","timestamp":1641947960829,"user_tz":-540,"elapsed":12,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["model = Sequential([\n","                    Dense(62, input_dim = 8, activation = 'relu'),\n","                    Dense(36, activation = 'relu'),\n","                    Dense(28, activation = 'relu'),\n","                    Dense(16, activation = 'relu'),\n","                    Dense(8, activation = 'relu'),\n","                    Dense(4, activation = 'relu'),\n","                    Dense(1, activation = 'sigmoid')\n","])\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yNpPgvwwugDc","executionInfo":{"status":"ok","timestamp":1641947960830,"user_tz":-540,"elapsed":12,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"outputId":"0f52b1ab-69bf-4144-95e6-aebd48983d76"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense (Dense)               (None, 62)                558       \n","                                                                 \n"," dense_1 (Dense)             (None, 36)                2268      \n","                                                                 \n"," dense_2 (Dense)             (None, 28)                1036      \n","                                                                 \n"," dense_3 (Dense)             (None, 16)                464       \n","                                                                 \n"," dense_4 (Dense)             (None, 8)                 136       \n","                                                                 \n"," dense_5 (Dense)             (None, 4)                 36        \n","                                                                 \n"," dense_6 (Dense)             (None, 1)                 5         \n","                                                                 \n","=================================================================\n","Total params: 4,503\n","Trainable params: 4,503\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"h6bYwk3HulL7","executionInfo":{"status":"ok","timestamp":1641947960830,"user_tz":-540,"elapsed":10,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## 모델 저장관련 설정"],"metadata":{"id":"PPcLr3d0CZWa"}},{"cell_type":"code","source":["import os\n","if not os.path.exists('model'):\n","    os.mkdir('model')"],"metadata":{"id":"l0SSdBJsumzz","executionInfo":{"status":"ok","timestamp":1641947960831,"user_tz":-540,"elapsed":11,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["model_path = 'model/titanic.h5'"],"metadata":{"id":"2m2dzda1unxS","executionInfo":{"status":"ok","timestamp":1641947960832,"user_tz":-540,"elapsed":11,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint\n","checkpoint = ModelCheckpoint(\n","    model_path, monitor='val_loss', verbose=1, save_best_only=True\n",")"],"metadata":{"id":"LNtDBNfdupLj","executionInfo":{"status":"ok","timestamp":1641947960832,"user_tz":-540,"elapsed":11,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# from tensorflow.keras.callbacks import EarlyStopping\n","# early_stopping = EarlyStopping(patience=100)"],"metadata":{"id":"nfg8Nyituq2b","executionInfo":{"status":"ok","timestamp":1641947960833,"user_tz":-540,"elapsed":12,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["## 모델 학습 및 저장"],"metadata":{"id":"PoVFFCStCczU"}},{"cell_type":"code","source":["hist = model.fit(\n","    X_train, y_train, validation_split=0.2, verbose=1,\n","    epochs=200, batch_size=210, callbacks=[checkpoint]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_LCs-HuNur2s","executionInfo":{"status":"ok","timestamp":1641947972247,"user_tz":-540,"elapsed":11425,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"outputId":"ca3b3b1f-b7f6-423f-890f-c1f299ed0f71"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","1/3 [=========>....................] - ETA: 1s - loss: 0.6960 - accuracy: 0.3333\n","Epoch 00001: val_loss improved from inf to 0.68903, saving model to model/titanic.h5\n","3/3 [==============================] - 1s 122ms/step - loss: 0.6903 - accuracy: 0.5237 - val_loss: 0.6890 - val_accuracy: 0.6364\n","Epoch 2/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.6814 - accuracy: 0.6857\n","Epoch 00002: val_loss improved from 0.68903 to 0.68296, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 34ms/step - loss: 0.6806 - accuracy: 0.6872 - val_loss: 0.6830 - val_accuracy: 0.6993\n","Epoch 3/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.6766 - accuracy: 0.7000\n","Epoch 00003: val_loss improved from 0.68296 to 0.67707, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 36ms/step - loss: 0.6720 - accuracy: 0.7135 - val_loss: 0.6771 - val_accuracy: 0.7273\n","Epoch 4/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.6675 - accuracy: 0.7190\n","Epoch 00004: val_loss improved from 0.67707 to 0.67029, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 36ms/step - loss: 0.6635 - accuracy: 0.7258 - val_loss: 0.6703 - val_accuracy: 0.7413\n","Epoch 5/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.6544 - accuracy: 0.8095\n","Epoch 00005: val_loss improved from 0.67029 to 0.66284, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 37ms/step - loss: 0.6534 - accuracy: 0.7627 - val_loss: 0.6628 - val_accuracy: 0.7483\n","Epoch 6/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.6475 - accuracy: 0.7857\n","Epoch 00006: val_loss improved from 0.66284 to 0.65389, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 33ms/step - loss: 0.6419 - accuracy: 0.7856 - val_loss: 0.6539 - val_accuracy: 0.7552\n","Epoch 7/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.6336 - accuracy: 0.7905\n","Epoch 00007: val_loss improved from 0.65389 to 0.64309, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 34ms/step - loss: 0.6291 - accuracy: 0.7856 - val_loss: 0.6431 - val_accuracy: 0.7762\n","Epoch 8/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.6191 - accuracy: 0.8048\n","Epoch 00008: val_loss improved from 0.64309 to 0.63127, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 36ms/step - loss: 0.6148 - accuracy: 0.7961 - val_loss: 0.6313 - val_accuracy: 0.7622\n","Epoch 9/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.6084 - accuracy: 0.7857\n","Epoch 00009: val_loss improved from 0.63127 to 0.61858, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 34ms/step - loss: 0.6000 - accuracy: 0.7944 - val_loss: 0.6186 - val_accuracy: 0.7692\n","Epoch 10/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.5851 - accuracy: 0.8190\n","Epoch 00010: val_loss improved from 0.61858 to 0.60374, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 37ms/step - loss: 0.5843 - accuracy: 0.7996 - val_loss: 0.6037 - val_accuracy: 0.7762\n","Epoch 11/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.5545 - accuracy: 0.8381\n","Epoch 00011: val_loss improved from 0.60374 to 0.58830, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 32ms/step - loss: 0.5683 - accuracy: 0.7979 - val_loss: 0.5883 - val_accuracy: 0.7902\n","Epoch 12/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.5842 - accuracy: 0.7571\n","Epoch 00012: val_loss improved from 0.58830 to 0.57147, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 32ms/step - loss: 0.5500 - accuracy: 0.7961 - val_loss: 0.5715 - val_accuracy: 0.7832\n","Epoch 13/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.5448 - accuracy: 0.7905\n","Epoch 00013: val_loss improved from 0.57147 to 0.55521, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 33ms/step - loss: 0.5330 - accuracy: 0.7979 - val_loss: 0.5552 - val_accuracy: 0.7902\n","Epoch 14/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.5571 - accuracy: 0.7524\n","Epoch 00014: val_loss improved from 0.55521 to 0.53886, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 34ms/step - loss: 0.5149 - accuracy: 0.7979 - val_loss: 0.5389 - val_accuracy: 0.7902\n","Epoch 15/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.5077 - accuracy: 0.8000\n","Epoch 00015: val_loss improved from 0.53886 to 0.51878, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 39ms/step - loss: 0.4975 - accuracy: 0.7979 - val_loss: 0.5188 - val_accuracy: 0.7972\n","Epoch 16/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4782 - accuracy: 0.7952\n","Epoch 00016: val_loss improved from 0.51878 to 0.49960, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 31ms/step - loss: 0.4816 - accuracy: 0.7979 - val_loss: 0.4996 - val_accuracy: 0.8042\n","Epoch 17/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4338 - accuracy: 0.8571\n","Epoch 00017: val_loss improved from 0.49960 to 0.48432, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 33ms/step - loss: 0.4673 - accuracy: 0.7979 - val_loss: 0.4843 - val_accuracy: 0.7972\n","Epoch 18/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4612 - accuracy: 0.7952\n","Epoch 00018: val_loss improved from 0.48432 to 0.47049, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 34ms/step - loss: 0.4528 - accuracy: 0.8014 - val_loss: 0.4705 - val_accuracy: 0.8042\n","Epoch 19/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4145 - accuracy: 0.8286\n","Epoch 00019: val_loss improved from 0.47049 to 0.46205, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 37ms/step - loss: 0.4434 - accuracy: 0.8102 - val_loss: 0.4620 - val_accuracy: 0.8042\n","Epoch 20/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4287 - accuracy: 0.8333\n","Epoch 00020: val_loss improved from 0.46205 to 0.45236, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 35ms/step - loss: 0.4356 - accuracy: 0.8137 - val_loss: 0.4524 - val_accuracy: 0.8182\n","Epoch 21/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4126 - accuracy: 0.8143\n","Epoch 00021: val_loss improved from 0.45236 to 0.44677, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 35ms/step - loss: 0.4297 - accuracy: 0.8172 - val_loss: 0.4468 - val_accuracy: 0.8112\n","Epoch 22/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4139 - accuracy: 0.8238\n","Epoch 00022: val_loss improved from 0.44677 to 0.44517, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 36ms/step - loss: 0.4249 - accuracy: 0.8172 - val_loss: 0.4452 - val_accuracy: 0.8112\n","Epoch 23/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4462 - accuracy: 0.8333\n","Epoch 00023: val_loss did not improve from 0.44517\n","3/3 [==============================] - 0s 17ms/step - loss: 0.4231 - accuracy: 0.8225 - val_loss: 0.4472 - val_accuracy: 0.7972\n","Epoch 24/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3989 - accuracy: 0.8238\n","Epoch 00024: val_loss improved from 0.44517 to 0.43676, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 31ms/step - loss: 0.4197 - accuracy: 0.8243 - val_loss: 0.4368 - val_accuracy: 0.8112\n","Epoch 25/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4779 - accuracy: 0.7857\n","Epoch 00025: val_loss improved from 0.43676 to 0.43338, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 32ms/step - loss: 0.4160 - accuracy: 0.8243 - val_loss: 0.4334 - val_accuracy: 0.8042\n","Epoch 26/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4128 - accuracy: 0.8238\n","Epoch 00026: val_loss improved from 0.43338 to 0.42849, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 32ms/step - loss: 0.4133 - accuracy: 0.8260 - val_loss: 0.4285 - val_accuracy: 0.8112\n","Epoch 27/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4453 - accuracy: 0.8000\n","Epoch 00027: val_loss did not improve from 0.42849\n","3/3 [==============================] - 0s 14ms/step - loss: 0.4110 - accuracy: 0.8313 - val_loss: 0.4287 - val_accuracy: 0.8042\n","Epoch 28/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3728 - accuracy: 0.8333\n","Epoch 00028: val_loss did not improve from 0.42849\n","3/3 [==============================] - 0s 15ms/step - loss: 0.4091 - accuracy: 0.8278 - val_loss: 0.4297 - val_accuracy: 0.8042\n","Epoch 29/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3815 - accuracy: 0.8429\n","Epoch 00029: val_loss did not improve from 0.42849\n","3/3 [==============================] - 0s 15ms/step - loss: 0.4061 - accuracy: 0.8278 - val_loss: 0.4314 - val_accuracy: 0.8042\n","Epoch 30/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3919 - accuracy: 0.8238\n","Epoch 00030: val_loss did not improve from 0.42849\n","3/3 [==============================] - 0s 19ms/step - loss: 0.4044 - accuracy: 0.8278 - val_loss: 0.4339 - val_accuracy: 0.8042\n","Epoch 31/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4514 - accuracy: 0.8048\n","Epoch 00031: val_loss did not improve from 0.42849\n","3/3 [==============================] - 0s 17ms/step - loss: 0.4029 - accuracy: 0.8278 - val_loss: 0.4317 - val_accuracy: 0.8042\n","Epoch 32/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3801 - accuracy: 0.8381\n","Epoch 00032: val_loss improved from 0.42849 to 0.42709, saving model to model/titanic.h5\n","3/3 [==============================] - 0s 33ms/step - loss: 0.4011 - accuracy: 0.8278 - val_loss: 0.4271 - val_accuracy: 0.8042\n","Epoch 33/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4353 - accuracy: 0.8000\n","Epoch 00033: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.3996 - accuracy: 0.8295 - val_loss: 0.4272 - val_accuracy: 0.8042\n","Epoch 34/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4116 - accuracy: 0.8333\n","Epoch 00034: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 14ms/step - loss: 0.3980 - accuracy: 0.8278 - val_loss: 0.4304 - val_accuracy: 0.8042\n","Epoch 35/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4249 - accuracy: 0.7857\n","Epoch 00035: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3971 - accuracy: 0.8278 - val_loss: 0.4314 - val_accuracy: 0.8042\n","Epoch 36/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3844 - accuracy: 0.8333\n","Epoch 00036: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 14ms/step - loss: 0.3954 - accuracy: 0.8295 - val_loss: 0.4386 - val_accuracy: 0.7972\n","Epoch 37/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3665 - accuracy: 0.8571\n","Epoch 00037: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 14ms/step - loss: 0.3943 - accuracy: 0.8330 - val_loss: 0.4327 - val_accuracy: 0.7972\n","Epoch 38/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4117 - accuracy: 0.8333\n","Epoch 00038: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3919 - accuracy: 0.8313 - val_loss: 0.4314 - val_accuracy: 0.7972\n","Epoch 39/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4170 - accuracy: 0.8143\n","Epoch 00039: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3905 - accuracy: 0.8313 - val_loss: 0.4298 - val_accuracy: 0.7972\n","Epoch 40/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3392 - accuracy: 0.8571\n","Epoch 00040: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 14ms/step - loss: 0.3893 - accuracy: 0.8313 - val_loss: 0.4276 - val_accuracy: 0.7972\n","Epoch 41/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4092 - accuracy: 0.8095\n","Epoch 00041: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3881 - accuracy: 0.8313 - val_loss: 0.4287 - val_accuracy: 0.7972\n","Epoch 42/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4091 - accuracy: 0.8286\n","Epoch 00042: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3863 - accuracy: 0.8313 - val_loss: 0.4355 - val_accuracy: 0.7972\n","Epoch 43/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3852 - accuracy: 0.8381\n","Epoch 00043: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3858 - accuracy: 0.8348 - val_loss: 0.4385 - val_accuracy: 0.7972\n","Epoch 44/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4247 - accuracy: 0.8190\n","Epoch 00044: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3843 - accuracy: 0.8366 - val_loss: 0.4357 - val_accuracy: 0.7972\n","Epoch 45/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3972 - accuracy: 0.8381\n","Epoch 00045: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 19ms/step - loss: 0.3823 - accuracy: 0.8383 - val_loss: 0.4306 - val_accuracy: 0.7972\n","Epoch 46/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4065 - accuracy: 0.8143\n","Epoch 00046: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3827 - accuracy: 0.8366 - val_loss: 0.4298 - val_accuracy: 0.7972\n","Epoch 47/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3565 - accuracy: 0.8476\n","Epoch 00047: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 14ms/step - loss: 0.3800 - accuracy: 0.8401 - val_loss: 0.4385 - val_accuracy: 0.7902\n","Epoch 48/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3628 - accuracy: 0.8429\n","Epoch 00048: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3792 - accuracy: 0.8366 - val_loss: 0.4436 - val_accuracy: 0.7902\n","Epoch 49/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4031 - accuracy: 0.8048\n","Epoch 00049: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3784 - accuracy: 0.8348 - val_loss: 0.4419 - val_accuracy: 0.7902\n","Epoch 50/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3718 - accuracy: 0.8143\n","Epoch 00050: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3771 - accuracy: 0.8401 - val_loss: 0.4356 - val_accuracy: 0.7972\n","Epoch 51/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4117 - accuracy: 0.8190\n","Epoch 00051: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.3757 - accuracy: 0.8418 - val_loss: 0.4389 - val_accuracy: 0.7902\n","Epoch 52/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3401 - accuracy: 0.8667\n","Epoch 00052: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.3743 - accuracy: 0.8418 - val_loss: 0.4427 - val_accuracy: 0.7902\n","Epoch 53/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3698 - accuracy: 0.8429\n","Epoch 00053: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 20ms/step - loss: 0.3748 - accuracy: 0.8401 - val_loss: 0.4472 - val_accuracy: 0.7762\n","Epoch 54/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3653 - accuracy: 0.8429\n","Epoch 00054: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3732 - accuracy: 0.8401 - val_loss: 0.4372 - val_accuracy: 0.8042\n","Epoch 55/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4104 - accuracy: 0.8143\n","Epoch 00055: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3716 - accuracy: 0.8453 - val_loss: 0.4399 - val_accuracy: 0.8042\n","Epoch 56/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3786 - accuracy: 0.8429\n","Epoch 00056: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3705 - accuracy: 0.8453 - val_loss: 0.4426 - val_accuracy: 0.7972\n","Epoch 57/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3769 - accuracy: 0.8429\n","Epoch 00057: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3689 - accuracy: 0.8418 - val_loss: 0.4486 - val_accuracy: 0.7902\n","Epoch 58/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3098 - accuracy: 0.8762\n","Epoch 00058: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3681 - accuracy: 0.8436 - val_loss: 0.4494 - val_accuracy: 0.7972\n","Epoch 59/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3609 - accuracy: 0.8381\n","Epoch 00059: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3673 - accuracy: 0.8418 - val_loss: 0.4449 - val_accuracy: 0.8112\n","Epoch 60/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3411 - accuracy: 0.8524\n","Epoch 00060: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3670 - accuracy: 0.8489 - val_loss: 0.4386 - val_accuracy: 0.8112\n","Epoch 61/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3864 - accuracy: 0.8333\n","Epoch 00061: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 21ms/step - loss: 0.3659 - accuracy: 0.8489 - val_loss: 0.4456 - val_accuracy: 0.8112\n","Epoch 62/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4157 - accuracy: 0.8333\n","Epoch 00062: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3646 - accuracy: 0.8471 - val_loss: 0.4529 - val_accuracy: 0.8042\n","Epoch 63/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3803 - accuracy: 0.8381\n","Epoch 00063: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.3625 - accuracy: 0.8524 - val_loss: 0.4472 - val_accuracy: 0.8042\n","Epoch 64/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3473 - accuracy: 0.8571\n","Epoch 00064: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3633 - accuracy: 0.8506 - val_loss: 0.4443 - val_accuracy: 0.8112\n","Epoch 65/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3349 - accuracy: 0.8810\n","Epoch 00065: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3611 - accuracy: 0.8471 - val_loss: 0.4552 - val_accuracy: 0.8042\n","Epoch 66/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3995 - accuracy: 0.8286\n","Epoch 00066: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3598 - accuracy: 0.8524 - val_loss: 0.4573 - val_accuracy: 0.8042\n","Epoch 67/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3347 - accuracy: 0.8714\n","Epoch 00067: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3598 - accuracy: 0.8524 - val_loss: 0.4496 - val_accuracy: 0.8112\n","Epoch 68/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3632 - accuracy: 0.8381\n","Epoch 00068: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3578 - accuracy: 0.8489 - val_loss: 0.4517 - val_accuracy: 0.8112\n","Epoch 69/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3489 - accuracy: 0.8429\n","Epoch 00069: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3572 - accuracy: 0.8559 - val_loss: 0.4601 - val_accuracy: 0.8042\n","Epoch 70/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.4081 - accuracy: 0.8143\n","Epoch 00070: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3560 - accuracy: 0.8559 - val_loss: 0.4559 - val_accuracy: 0.8042\n","Epoch 71/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3729 - accuracy: 0.8619\n","Epoch 00071: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3541 - accuracy: 0.8559 - val_loss: 0.4591 - val_accuracy: 0.8042\n","Epoch 72/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3431 - accuracy: 0.8429\n","Epoch 00072: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3545 - accuracy: 0.8524 - val_loss: 0.4610 - val_accuracy: 0.8042\n","Epoch 73/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3471 - accuracy: 0.8714\n","Epoch 00073: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3527 - accuracy: 0.8559 - val_loss: 0.4617 - val_accuracy: 0.8042\n","Epoch 74/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3265 - accuracy: 0.8810\n","Epoch 00074: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3517 - accuracy: 0.8576 - val_loss: 0.4615 - val_accuracy: 0.8042\n","Epoch 75/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3639 - accuracy: 0.8571\n","Epoch 00075: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3518 - accuracy: 0.8576 - val_loss: 0.4668 - val_accuracy: 0.7972\n","Epoch 76/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3523 - accuracy: 0.8429\n","Epoch 00076: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 28ms/step - loss: 0.3502 - accuracy: 0.8576 - val_loss: 0.4569 - val_accuracy: 0.8112\n","Epoch 77/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3219 - accuracy: 0.8714\n","Epoch 00077: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3487 - accuracy: 0.8559 - val_loss: 0.4600 - val_accuracy: 0.8042\n","Epoch 78/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3325 - accuracy: 0.8571\n","Epoch 00078: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3482 - accuracy: 0.8559 - val_loss: 0.4704 - val_accuracy: 0.7972\n","Epoch 79/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3038 - accuracy: 0.8857\n","Epoch 00079: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3480 - accuracy: 0.8576 - val_loss: 0.4699 - val_accuracy: 0.7972\n","Epoch 80/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3535 - accuracy: 0.8619\n","Epoch 00080: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3448 - accuracy: 0.8576 - val_loss: 0.4623 - val_accuracy: 0.8112\n","Epoch 81/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3396 - accuracy: 0.8571\n","Epoch 00081: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3446 - accuracy: 0.8594 - val_loss: 0.4594 - val_accuracy: 0.8112\n","Epoch 82/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3345 - accuracy: 0.8667\n","Epoch 00082: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3445 - accuracy: 0.8594 - val_loss: 0.4690 - val_accuracy: 0.8042\n","Epoch 83/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3388 - accuracy: 0.8476\n","Epoch 00083: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3423 - accuracy: 0.8576 - val_loss: 0.4701 - val_accuracy: 0.7972\n","Epoch 84/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3495 - accuracy: 0.8429\n","Epoch 00084: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3413 - accuracy: 0.8594 - val_loss: 0.4745 - val_accuracy: 0.7972\n","Epoch 85/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3521 - accuracy: 0.8429\n","Epoch 00085: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3402 - accuracy: 0.8594 - val_loss: 0.4716 - val_accuracy: 0.7972\n","Epoch 86/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3375 - accuracy: 0.8714\n","Epoch 00086: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3391 - accuracy: 0.8576 - val_loss: 0.4690 - val_accuracy: 0.7972\n","Epoch 87/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3327 - accuracy: 0.8571\n","Epoch 00087: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3386 - accuracy: 0.8612 - val_loss: 0.4652 - val_accuracy: 0.8112\n","Epoch 88/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3172 - accuracy: 0.8619\n","Epoch 00088: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3376 - accuracy: 0.8647 - val_loss: 0.4748 - val_accuracy: 0.7972\n","Epoch 89/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3196 - accuracy: 0.8810\n","Epoch 00089: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3375 - accuracy: 0.8629 - val_loss: 0.4858 - val_accuracy: 0.7902\n","Epoch 90/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3417 - accuracy: 0.8476\n","Epoch 00090: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3352 - accuracy: 0.8629 - val_loss: 0.4704 - val_accuracy: 0.8042\n","Epoch 91/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2950 - accuracy: 0.9048\n","Epoch 00091: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3385 - accuracy: 0.8682 - val_loss: 0.4650 - val_accuracy: 0.8182\n","Epoch 92/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3312 - accuracy: 0.8667\n","Epoch 00092: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3337 - accuracy: 0.8682 - val_loss: 0.4903 - val_accuracy: 0.7972\n","Epoch 93/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2850 - accuracy: 0.9048\n","Epoch 00093: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3365 - accuracy: 0.8664 - val_loss: 0.4888 - val_accuracy: 0.7972\n","Epoch 94/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3038 - accuracy: 0.8619\n","Epoch 00094: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3344 - accuracy: 0.8612 - val_loss: 0.4685 - val_accuracy: 0.8112\n","Epoch 95/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3513 - accuracy: 0.8524\n","Epoch 00095: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3327 - accuracy: 0.8699 - val_loss: 0.4804 - val_accuracy: 0.8042\n","Epoch 96/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2875 - accuracy: 0.8952\n","Epoch 00096: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3328 - accuracy: 0.8682 - val_loss: 0.4974 - val_accuracy: 0.7902\n","Epoch 97/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3625 - accuracy: 0.8524\n","Epoch 00097: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.3311 - accuracy: 0.8647 - val_loss: 0.4822 - val_accuracy: 0.8042\n","Epoch 98/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2847 - accuracy: 0.8810\n","Epoch 00098: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3286 - accuracy: 0.8682 - val_loss: 0.4759 - val_accuracy: 0.8112\n","Epoch 99/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3484 - accuracy: 0.8619\n","Epoch 00099: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3299 - accuracy: 0.8717 - val_loss: 0.4891 - val_accuracy: 0.8042\n","Epoch 100/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3248 - accuracy: 0.8524\n","Epoch 00100: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3267 - accuracy: 0.8699 - val_loss: 0.5006 - val_accuracy: 0.7902\n","Epoch 101/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3432 - accuracy: 0.8619\n","Epoch 00101: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3281 - accuracy: 0.8647 - val_loss: 0.4951 - val_accuracy: 0.7972\n","Epoch 102/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2873 - accuracy: 0.8667\n","Epoch 00102: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3238 - accuracy: 0.8629 - val_loss: 0.4827 - val_accuracy: 0.8112\n","Epoch 103/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3623 - accuracy: 0.8333\n","Epoch 00103: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 21ms/step - loss: 0.3260 - accuracy: 0.8647 - val_loss: 0.4916 - val_accuracy: 0.8042\n","Epoch 104/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3729 - accuracy: 0.8429\n","Epoch 00104: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.3216 - accuracy: 0.8664 - val_loss: 0.5027 - val_accuracy: 0.7972\n","Epoch 105/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3167 - accuracy: 0.8714\n","Epoch 00105: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3231 - accuracy: 0.8682 - val_loss: 0.5019 - val_accuracy: 0.7972\n","Epoch 106/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3558 - accuracy: 0.8476\n","Epoch 00106: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3212 - accuracy: 0.8735 - val_loss: 0.4908 - val_accuracy: 0.7972\n","Epoch 107/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3491 - accuracy: 0.8476\n","Epoch 00107: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3203 - accuracy: 0.8682 - val_loss: 0.4947 - val_accuracy: 0.8042\n","Epoch 108/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3897 - accuracy: 0.8381\n","Epoch 00108: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3179 - accuracy: 0.8682 - val_loss: 0.4992 - val_accuracy: 0.7902\n","Epoch 109/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2774 - accuracy: 0.8905\n","Epoch 00109: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3175 - accuracy: 0.8664 - val_loss: 0.5029 - val_accuracy: 0.7902\n","Epoch 110/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3019 - accuracy: 0.8619\n","Epoch 00110: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3166 - accuracy: 0.8735 - val_loss: 0.4985 - val_accuracy: 0.8042\n","Epoch 111/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3164 - accuracy: 0.8667\n","Epoch 00111: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3144 - accuracy: 0.8752 - val_loss: 0.5029 - val_accuracy: 0.7972\n","Epoch 112/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3316 - accuracy: 0.8571\n","Epoch 00112: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3121 - accuracy: 0.8735 - val_loss: 0.5069 - val_accuracy: 0.7972\n","Epoch 113/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3299 - accuracy: 0.8571\n","Epoch 00113: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 22ms/step - loss: 0.3114 - accuracy: 0.8717 - val_loss: 0.5099 - val_accuracy: 0.8042\n","Epoch 114/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3381 - accuracy: 0.8524\n","Epoch 00114: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.3107 - accuracy: 0.8699 - val_loss: 0.5115 - val_accuracy: 0.7972\n","Epoch 115/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3099 - accuracy: 0.8714\n","Epoch 00115: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3086 - accuracy: 0.8752 - val_loss: 0.5080 - val_accuracy: 0.8042\n","Epoch 116/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3339 - accuracy: 0.8524\n","Epoch 00116: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3076 - accuracy: 0.8752 - val_loss: 0.5210 - val_accuracy: 0.8112\n","Epoch 117/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2945 - accuracy: 0.8857\n","Epoch 00117: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 19ms/step - loss: 0.3077 - accuracy: 0.8770 - val_loss: 0.5226 - val_accuracy: 0.8112\n","Epoch 118/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2998 - accuracy: 0.8762\n","Epoch 00118: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.3053 - accuracy: 0.8752 - val_loss: 0.5131 - val_accuracy: 0.7902\n","Epoch 119/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3081 - accuracy: 0.8762\n","Epoch 00119: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3045 - accuracy: 0.8805 - val_loss: 0.5304 - val_accuracy: 0.7972\n","Epoch 120/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3586 - accuracy: 0.8619\n","Epoch 00120: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.3029 - accuracy: 0.8735 - val_loss: 0.5249 - val_accuracy: 0.8042\n","Epoch 121/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2731 - accuracy: 0.8810\n","Epoch 00121: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3069 - accuracy: 0.8717 - val_loss: 0.5250 - val_accuracy: 0.7972\n","Epoch 122/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3144 - accuracy: 0.8571\n","Epoch 00122: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 22ms/step - loss: 0.3011 - accuracy: 0.8770 - val_loss: 0.5595 - val_accuracy: 0.8112\n","Epoch 123/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3078 - accuracy: 0.8714\n","Epoch 00123: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3057 - accuracy: 0.8717 - val_loss: 0.5300 - val_accuracy: 0.8112\n","Epoch 124/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3311 - accuracy: 0.8619\n","Epoch 00124: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2997 - accuracy: 0.8822 - val_loss: 0.5415 - val_accuracy: 0.8042\n","Epoch 125/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2976 - accuracy: 0.8857\n","Epoch 00125: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2961 - accuracy: 0.8787 - val_loss: 0.5597 - val_accuracy: 0.8112\n","Epoch 126/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2794 - accuracy: 0.8857\n","Epoch 00126: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2976 - accuracy: 0.8822 - val_loss: 0.5404 - val_accuracy: 0.7972\n","Epoch 127/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2621 - accuracy: 0.9190\n","Epoch 00127: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2973 - accuracy: 0.8822 - val_loss: 0.5465 - val_accuracy: 0.8042\n","Epoch 128/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3266 - accuracy: 0.8524\n","Epoch 00128: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.2929 - accuracy: 0.8822 - val_loss: 0.5723 - val_accuracy: 0.8042\n","Epoch 129/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3301 - accuracy: 0.8524\n","Epoch 00129: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2957 - accuracy: 0.8770 - val_loss: 0.5531 - val_accuracy: 0.7972\n","Epoch 130/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3016 - accuracy: 0.8857\n","Epoch 00130: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2908 - accuracy: 0.8840 - val_loss: 0.5580 - val_accuracy: 0.8042\n","Epoch 131/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2341 - accuracy: 0.9238\n","Epoch 00131: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2913 - accuracy: 0.8822 - val_loss: 0.5663 - val_accuracy: 0.8112\n","Epoch 132/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3048 - accuracy: 0.8714\n","Epoch 00132: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 19ms/step - loss: 0.2895 - accuracy: 0.8875 - val_loss: 0.5483 - val_accuracy: 0.8042\n","Epoch 133/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3054 - accuracy: 0.8810\n","Epoch 00133: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 14ms/step - loss: 0.2885 - accuracy: 0.8858 - val_loss: 0.5528 - val_accuracy: 0.8042\n","Epoch 134/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2732 - accuracy: 0.8857\n","Epoch 00134: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.2857 - accuracy: 0.8840 - val_loss: 0.5724 - val_accuracy: 0.8112\n","Epoch 135/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2957 - accuracy: 0.8857\n","Epoch 00135: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 14ms/step - loss: 0.2870 - accuracy: 0.8875 - val_loss: 0.5636 - val_accuracy: 0.8042\n","Epoch 136/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2808 - accuracy: 0.9000\n","Epoch 00136: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2854 - accuracy: 0.8875 - val_loss: 0.5734 - val_accuracy: 0.8112\n","Epoch 137/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2760 - accuracy: 0.9000\n","Epoch 00137: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.2835 - accuracy: 0.8875 - val_loss: 0.5788 - val_accuracy: 0.8112\n","Epoch 138/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2721 - accuracy: 0.8762\n","Epoch 00138: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2817 - accuracy: 0.8840 - val_loss: 0.5635 - val_accuracy: 0.8042\n","Epoch 139/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3217 - accuracy: 0.8714\n","Epoch 00139: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2819 - accuracy: 0.8858 - val_loss: 0.5715 - val_accuracy: 0.8112\n","Epoch 140/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2973 - accuracy: 0.8857\n","Epoch 00140: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 19ms/step - loss: 0.2815 - accuracy: 0.8822 - val_loss: 0.5811 - val_accuracy: 0.8112\n","Epoch 141/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2606 - accuracy: 0.9000\n","Epoch 00141: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2792 - accuracy: 0.8875 - val_loss: 0.5773 - val_accuracy: 0.8042\n","Epoch 142/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2533 - accuracy: 0.9000\n","Epoch 00142: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2790 - accuracy: 0.8893 - val_loss: 0.5956 - val_accuracy: 0.8112\n","Epoch 143/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2407 - accuracy: 0.9286\n","Epoch 00143: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2767 - accuracy: 0.8893 - val_loss: 0.5982 - val_accuracy: 0.8112\n","Epoch 144/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2770 - accuracy: 0.8905\n","Epoch 00144: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.2778 - accuracy: 0.8893 - val_loss: 0.5859 - val_accuracy: 0.8042\n","Epoch 145/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2877 - accuracy: 0.8810\n","Epoch 00145: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 20ms/step - loss: 0.2774 - accuracy: 0.8893 - val_loss: 0.5989 - val_accuracy: 0.8252\n","Epoch 146/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2509 - accuracy: 0.9095\n","Epoch 00146: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2759 - accuracy: 0.8928 - val_loss: 0.5996 - val_accuracy: 0.8112\n","Epoch 147/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2823 - accuracy: 0.8810\n","Epoch 00147: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2749 - accuracy: 0.8928 - val_loss: 0.5987 - val_accuracy: 0.8112\n","Epoch 148/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2595 - accuracy: 0.9048\n","Epoch 00148: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2739 - accuracy: 0.8893 - val_loss: 0.5928 - val_accuracy: 0.8042\n","Epoch 149/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3170 - accuracy: 0.8524\n","Epoch 00149: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2758 - accuracy: 0.8822 - val_loss: 0.6377 - val_accuracy: 0.8322\n","Epoch 150/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3109 - accuracy: 0.8667\n","Epoch 00150: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2805 - accuracy: 0.8840 - val_loss: 0.6274 - val_accuracy: 0.8112\n","Epoch 151/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2914 - accuracy: 0.8714\n","Epoch 00151: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.2741 - accuracy: 0.8805 - val_loss: 0.5970 - val_accuracy: 0.7972\n","Epoch 152/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2857 - accuracy: 0.8667\n","Epoch 00152: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2764 - accuracy: 0.8805 - val_loss: 0.6432 - val_accuracy: 0.8182\n","Epoch 153/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3015 - accuracy: 0.8714\n","Epoch 00153: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2766 - accuracy: 0.8822 - val_loss: 0.6300 - val_accuracy: 0.8252\n","Epoch 154/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2444 - accuracy: 0.8952\n","Epoch 00154: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.2814 - accuracy: 0.8735 - val_loss: 0.6251 - val_accuracy: 0.8112\n","Epoch 155/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2119 - accuracy: 0.9238\n","Epoch 00155: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2731 - accuracy: 0.8875 - val_loss: 0.6626 - val_accuracy: 0.8182\n","Epoch 156/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2606 - accuracy: 0.9000\n","Epoch 00156: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.2725 - accuracy: 0.8858 - val_loss: 0.6005 - val_accuracy: 0.7902\n","Epoch 157/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2456 - accuracy: 0.9048\n","Epoch 00157: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2738 - accuracy: 0.8787 - val_loss: 0.6192 - val_accuracy: 0.8252\n","Epoch 158/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3148 - accuracy: 0.8714\n","Epoch 00158: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2739 - accuracy: 0.8858 - val_loss: 0.6562 - val_accuracy: 0.8252\n","Epoch 159/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2722 - accuracy: 0.8810\n","Epoch 00159: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2657 - accuracy: 0.8946 - val_loss: 0.6092 - val_accuracy: 0.8112\n","Epoch 160/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2365 - accuracy: 0.9143\n","Epoch 00160: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2734 - accuracy: 0.8946 - val_loss: 0.6319 - val_accuracy: 0.8182\n","Epoch 161/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2679 - accuracy: 0.8952\n","Epoch 00161: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2659 - accuracy: 0.8875 - val_loss: 0.6637 - val_accuracy: 0.8252\n","Epoch 162/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2673 - accuracy: 0.8905\n","Epoch 00162: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 23ms/step - loss: 0.2662 - accuracy: 0.8946 - val_loss: 0.6362 - val_accuracy: 0.8252\n","Epoch 163/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2875 - accuracy: 0.8810\n","Epoch 00163: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.2657 - accuracy: 0.8946 - val_loss: 0.6458 - val_accuracy: 0.8252\n","Epoch 164/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2397 - accuracy: 0.9095\n","Epoch 00164: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2680 - accuracy: 0.8858 - val_loss: 0.6500 - val_accuracy: 0.8252\n","Epoch 165/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3444 - accuracy: 0.8476\n","Epoch 00165: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2660 - accuracy: 0.8946 - val_loss: 0.6370 - val_accuracy: 0.8322\n","Epoch 166/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2822 - accuracy: 0.8762\n","Epoch 00166: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 21ms/step - loss: 0.2640 - accuracy: 0.8963 - val_loss: 0.6707 - val_accuracy: 0.8252\n","Epoch 167/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2774 - accuracy: 0.8810\n","Epoch 00167: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 19ms/step - loss: 0.2642 - accuracy: 0.8910 - val_loss: 0.6488 - val_accuracy: 0.8252\n","Epoch 168/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2791 - accuracy: 0.8905\n","Epoch 00168: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 21ms/step - loss: 0.2630 - accuracy: 0.8963 - val_loss: 0.6364 - val_accuracy: 0.8182\n","Epoch 169/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2539 - accuracy: 0.9048\n","Epoch 00169: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2632 - accuracy: 0.8981 - val_loss: 0.6808 - val_accuracy: 0.8252\n","Epoch 170/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2897 - accuracy: 0.8810\n","Epoch 00170: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2623 - accuracy: 0.8981 - val_loss: 0.6665 - val_accuracy: 0.8182\n","Epoch 171/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2464 - accuracy: 0.8952\n","Epoch 00171: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2607 - accuracy: 0.8963 - val_loss: 0.6647 - val_accuracy: 0.8182\n","Epoch 172/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2472 - accuracy: 0.9048\n","Epoch 00172: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2613 - accuracy: 0.8981 - val_loss: 0.6639 - val_accuracy: 0.8252\n","Epoch 173/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2584 - accuracy: 0.9000\n","Epoch 00173: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2568 - accuracy: 0.8981 - val_loss: 0.6973 - val_accuracy: 0.8182\n","Epoch 174/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.3000 - accuracy: 0.8810\n","Epoch 00174: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2620 - accuracy: 0.8998 - val_loss: 0.6682 - val_accuracy: 0.8182\n","Epoch 175/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2450 - accuracy: 0.9048\n","Epoch 00175: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.2561 - accuracy: 0.9016 - val_loss: 0.6846 - val_accuracy: 0.8252\n","Epoch 176/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2800 - accuracy: 0.8952\n","Epoch 00176: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2562 - accuracy: 0.8981 - val_loss: 0.6759 - val_accuracy: 0.8182\n","Epoch 177/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2665 - accuracy: 0.9048\n","Epoch 00177: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2560 - accuracy: 0.8928 - val_loss: 0.6967 - val_accuracy: 0.8112\n","Epoch 178/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2498 - accuracy: 0.9000\n","Epoch 00178: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2570 - accuracy: 0.8998 - val_loss: 0.6984 - val_accuracy: 0.8252\n","Epoch 179/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2992 - accuracy: 0.8810\n","Epoch 00179: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2570 - accuracy: 0.8928 - val_loss: 0.6678 - val_accuracy: 0.8182\n","Epoch 180/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2511 - accuracy: 0.9095\n","Epoch 00180: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2558 - accuracy: 0.9033 - val_loss: 0.7095 - val_accuracy: 0.8182\n","Epoch 181/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2691 - accuracy: 0.8857\n","Epoch 00181: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2554 - accuracy: 0.9033 - val_loss: 0.6906 - val_accuracy: 0.8182\n","Epoch 182/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2685 - accuracy: 0.8762\n","Epoch 00182: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2543 - accuracy: 0.8928 - val_loss: 0.7000 - val_accuracy: 0.8182\n","Epoch 183/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2391 - accuracy: 0.9238\n","Epoch 00183: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2530 - accuracy: 0.9016 - val_loss: 0.7057 - val_accuracy: 0.8322\n","Epoch 184/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2576 - accuracy: 0.9048\n","Epoch 00184: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2513 - accuracy: 0.9033 - val_loss: 0.6808 - val_accuracy: 0.8252\n","Epoch 185/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2409 - accuracy: 0.9000\n","Epoch 00185: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2541 - accuracy: 0.9016 - val_loss: 0.7063 - val_accuracy: 0.8112\n","Epoch 186/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2999 - accuracy: 0.8714\n","Epoch 00186: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2518 - accuracy: 0.8981 - val_loss: 0.7158 - val_accuracy: 0.8252\n","Epoch 187/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2240 - accuracy: 0.9286\n","Epoch 00187: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2515 - accuracy: 0.9051 - val_loss: 0.7062 - val_accuracy: 0.8042\n","Epoch 188/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2642 - accuracy: 0.8905\n","Epoch 00188: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2511 - accuracy: 0.9016 - val_loss: 0.7183 - val_accuracy: 0.8252\n","Epoch 189/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2619 - accuracy: 0.9095\n","Epoch 00189: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.2494 - accuracy: 0.9016 - val_loss: 0.7082 - val_accuracy: 0.8182\n","Epoch 190/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2255 - accuracy: 0.9238\n","Epoch 00190: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 15ms/step - loss: 0.2518 - accuracy: 0.9016 - val_loss: 0.7248 - val_accuracy: 0.8042\n","Epoch 191/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2419 - accuracy: 0.9143\n","Epoch 00191: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 20ms/step - loss: 0.2514 - accuracy: 0.8998 - val_loss: 0.7495 - val_accuracy: 0.8252\n","Epoch 192/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2334 - accuracy: 0.9000\n","Epoch 00192: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 19ms/step - loss: 0.2510 - accuracy: 0.9051 - val_loss: 0.7221 - val_accuracy: 0.8182\n","Epoch 193/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2184 - accuracy: 0.9238\n","Epoch 00193: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2485 - accuracy: 0.9121 - val_loss: 0.7161 - val_accuracy: 0.8182\n","Epoch 194/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2230 - accuracy: 0.9238\n","Epoch 00194: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2465 - accuracy: 0.9104 - val_loss: 0.7284 - val_accuracy: 0.8182\n","Epoch 195/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2829 - accuracy: 0.8619\n","Epoch 00195: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2531 - accuracy: 0.8946 - val_loss: 0.7315 - val_accuracy: 0.8182\n","Epoch 196/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2398 - accuracy: 0.8905\n","Epoch 00196: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2460 - accuracy: 0.9033 - val_loss: 0.7149 - val_accuracy: 0.8112\n","Epoch 197/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2714 - accuracy: 0.8857\n","Epoch 00197: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2497 - accuracy: 0.9033 - val_loss: 0.7650 - val_accuracy: 0.8182\n","Epoch 198/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2593 - accuracy: 0.8952\n","Epoch 00198: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2477 - accuracy: 0.8998 - val_loss: 0.7338 - val_accuracy: 0.8182\n","Epoch 199/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2364 - accuracy: 0.9143\n","Epoch 00199: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2461 - accuracy: 0.8998 - val_loss: 0.7585 - val_accuracy: 0.8322\n","Epoch 200/200\n","1/3 [=========>....................] - ETA: 0s - loss: 0.2780 - accuracy: 0.8857\n","Epoch 00200: val_loss did not improve from 0.42709\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2468 - accuracy: 0.8981 - val_loss: 0.7484 - val_accuracy: 0.8182\n"]}]},{"cell_type":"markdown","source":["## 평가 - 베스트 모델"],"metadata":{"id":"LfJ1HxaACiUW"}},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","best_model=load_model(model_path)\n","best_model.evaluate(X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NGjq94EDutlM","executionInfo":{"status":"ok","timestamp":1641947972248,"user_tz":-540,"elapsed":16,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}},"outputId":"4a5a5a56-6dfc-4245-fdf2-01f9ddbb0286"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["6/6 [==============================] - 0s 3ms/step - loss: 0.4634 - accuracy: 0.8212\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.46344244480133057, 0.8212290406227112]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":[""],"metadata":{"id":"KmV9EtSZtuY_","executionInfo":{"status":"ok","timestamp":1641947972248,"user_tz":-540,"elapsed":14,"user":{"displayName":"딱한준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfN-AMjPbGCVSebxJdq--WoaejAVoUZk_zfsgkAA=s64","userId":"01446447523526388458"}}},"execution_count":24,"outputs":[]}]}